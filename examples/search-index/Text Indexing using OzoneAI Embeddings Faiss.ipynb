{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d8240b8",
   "metadata": {},
   "source": [
    "### Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb5d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install below libraries if don't if you are trying for the first time.\n",
    "# !pip install langchain\n",
    "# !pip install numpy\n",
    "# !pip install faiss-cpu\n",
    "# !pip install requests\n",
    "# !pip install tqdm\n",
    "# !pip install ozonetel-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3333337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np, faiss, sqlite3, requests, os, json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d6a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentDatabase:\n",
    "    def __init__(self, db_file):\n",
    "        self.db_file = db_file\n",
    "\n",
    "    def _create_table(self):\n",
    "        self.cursor.execute('''CREATE TABLE IF NOT EXISTS documents\n",
    "                              (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                               content TEXT,\n",
    "                               UNIQUE(id) ON CONFLICT IGNORE)''')\n",
    "        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_id ON documents (id)')\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.conn = sqlite3.connect(self.db_file)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self._create_table()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.conn.close()\n",
    "        \n",
    "    def insert_document(self, document):\n",
    "        self.cursor.execute(\"INSERT INTO documents (content) VALUES (?)\", (document,))\n",
    "        self.conn.commit()\n",
    "\n",
    "    def select_documents(self, query):\n",
    "        self.cursor.execute(query)\n",
    "        return self.cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4851a00b",
   "metadata": {},
   "source": [
    "### Define credential in environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8d2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OZAI_API_CREDENTIALS\"] = \"./cred.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c217280f",
   "metadata": {},
   "source": [
    "### Read text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b124615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path=\"./sample.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e81957",
   "metadata": {},
   "source": [
    "### Preprocess text document using langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text document and split by chunk size\n",
    "# Note: Document handler can be changed based on usage (check more options https://python.langchain.com/docs/modules/data_connection/document_loaders/)\n",
    "\n",
    "# load\n",
    "text_loader = TextLoader(text_path)\n",
    "documents = text_loader.load()\n",
    "\n",
    "# split document\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=10)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee95e4cb",
   "metadata": {},
   "source": [
    "### Encoding documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff44e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ozoneai.embeddings import list_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fe7b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# endcoder_modelid = \"paraphrase-multilingual-mpnet-base-v2\" \n",
    "# model = \"siv-sentence-bitnet-pmbv2-wikid-large\"\n",
    "\n",
    "endcoder_modelid = \"BAAI/bge-m3\"\n",
    "model = \"sieve-bge-m3-en-aug-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34744cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the documents\n",
    "\n",
    "# Import `BinarizeSentenceEmbedding` class from the `ozoneai.embeder` module.\n",
    "from ozoneai.embeddings import BinarizeSentenceEmbedding\n",
    "\n",
    "batch_size = 20\n",
    "# Extract Embeddings: Use the `binarize` method to obtain binarized embeddings for given texts .\n",
    "# Supported models encoders are `sentence-transformers/paraphrase-multilingual-mpnet-base-v2` and `BAAI/bge-m3`\n",
    "# Alternatively if you have stored these models in local directory you can use like `/path/to/paraphrase-multilingual-mpnet-base-v2` or `/path/to/bge-m3`\n",
    "with BinarizeSentenceEmbedding(\n",
    "    endcoder_modelid=endcoder_modelid) as embedder:\n",
    "    \n",
    "    ndocs = len(docs)\n",
    "    encoded_documents = []\n",
    "    for i in tqdm(range(0, ndocs, batch_size)):\n",
    "        d = docs[i:min(i+batch_size, ndocs)]\n",
    "        d = [di.page_content for di in d]\n",
    "        emb = embedder.encode(d)\n",
    "        emb_binarized = embedder.binarize(emb, model=model) # max limit 20 vectors per request\n",
    "        encoded_documents.append(emb_binarized.embedding)\n",
    "\n",
    "encoded_documents = np.concatenate(encoded_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a758fa",
   "metadata": {},
   "source": [
    "### Create index using Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743ca555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm -rvf index*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e230ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoded documents are packed bit ('unit8')\n",
    "# make sure it fits to your RAM\n",
    "print(f\"embedding size: {encoded_documents.shape}\")\n",
    "\n",
    "# Actual embedding dimension would be 8 times as data is uint8\n",
    "\n",
    "dimension = encoded_documents.shape[1] * 8  # Dimension of the binary vectors\n",
    "\n",
    "# Create faiss binary index\n",
    "index = faiss.IndexBinaryFlat(dimension)\n",
    "\n",
    "# Add the binary vectors to the index\n",
    "# Note: avoid duplicate data insert\n",
    "index.add(encoded_documents)\n",
    "\n",
    "# persist data and text\n",
    "faiss.write_index_binary(index, 'index.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30798d73",
   "metadata": {},
   "source": [
    "### Storing document in sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a36087",
   "metadata": {},
   "outputs": [],
   "source": [
    "with DocumentDatabase('index.db') as conn:\n",
    "    \n",
    "    # Insert the documents into the database\n",
    "    for doc in tqdm(docs):\n",
    "        conn.insert_document(doc.page_content)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1143a32e",
   "metadata": {},
   "source": [
    "### Query Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f80555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a search on the index\n",
    "query = \"What is positional embeddings?\"\n",
    "\n",
    "with BinarizeSentenceEmbedding(\n",
    "    endcoder_modelid=endcoder_modelid) as embedder, DocumentDatabase('index.db') as conn:\n",
    "    \n",
    "    emb = embedder.encode(query)\n",
    "    emb_binarized = embedder.binarize(emb, model=model)\n",
    "    \n",
    "    encoded_query = emb_binarized.embedding\n",
    "\n",
    "    D, I = index.search(encoded_query, k=5)  # Retrieve top 5 most similar documents\n",
    "\n",
    "    selected_data = [conn.select_documents(f\"select * from documents where id={i};\")[0] for i in I[0]]\n",
    "    for i, s in enumerate(selected_data):\n",
    "        print(f\"\"\"\n",
    "        Query: {query}\\n\n",
    "        --------------------------\n",
    "        Nearest [{i}], DocID [{s[0]}]:\\n\n",
    "        Text: {s[1]}\n",
    "        \n",
    "        xxxxxxx\n",
    "        \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
